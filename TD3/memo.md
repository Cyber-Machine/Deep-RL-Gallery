Double DQNは有用だがActor-Criticには使えない問題


そこでDouble Q-Learningを採用

https://arxiv.org/abs/1509.06461


真のQ関数で最適化されたポリシーと近似Q関数で最適化されたポリシーで近似Q関数の期待値をとったら近似Q関数で最適化されたポリシーのほうが期待値が大きくなる。近似Q関数ポリシーは近似Q関数を最大化するように訓練されているのだから当然である

逆もまた然り


True estimation はそのときのポリシーで？


- 遅延更新

ポリシーが破綻すると連鎖破綻しやすいのでvalueの推定を安定させたい


- ノイズ正則化

推定値のスパイクに対してOverfittingする

targetの見積りでアクションにノイズを加えることで学習を安定させる。
感覚的には画像分類でを画像を歪ませたりしても同じものと認識できるようにするのと似ている

連続値制御だからできるテクニック


https://github.com/sfujim/TD3
